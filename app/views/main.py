# # -*- coding: utf-8 -*-
# """main.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1bgKtY5S6J81aI5CvxJCJM2OALjLzwooR
# """


# from google.colab import drive
# drive.mount('/content/drive')

# import numpy as np
# import pandas as pd
# import pickle

# !pip uninstall scikit-learn -y
# !pip install -U scikit-learn

# import sklearn
# sklearn.__version__

# # Function to generate a cumulative DataFrames with all Batter Match Data and Bowler Match data
# def make_df(path, var):

#   data = []

#   for i in range(254):
#     file_path = path+"/Match"+str(i)+var+".csv"
#     temp_df = pd.read_csv(file_path)

#     for i in temp_df.values:
#       data.append(i[1:])

#   return data

# Batter_path = "/content/drive/MyDrive/PROJECT/Batter_Match_Data"
# Bowler_path = "/content/drive/MyDrive/PROJECT/Bowler_Match_Data"

# Batter_data = make_df(Batter_path, "Bat")
# Bowler_data = make_df(Bowler_path, "Bowl")

# Train_bat = pd.DataFrame(data = Batter_data, columns = ['Player','Mat','Inns','Runs','HS','Ave','BF','SR','100','50','0','4s','6s','IsInD11'])
# Train_bowl = pd.DataFrame(data = Bowler_data, columns = ['Player','Mat','Inns','Overs','Mdns','Runs','Wkts','Ave','Econ','SR','4','5','IsInD11'])

# Train_bowl

# """# **EXPLPORATORY DATA ANALYSIS**"""

# Train_bat.isnull().sum()

# Train_bat.info()

# Train_bat.describe()

# Train_bowl.describe()

# """# **DATA PREPROCESSING**"""

# Train_bat  = Train_bat.drop(columns = ["Player"])
# Train_bowl  = Train_bowl.drop(columns = ["Player"])

# X_bat = Train_bat[['Mat','Inns','Runs','HS','Ave','BF','SR','100','50','0','4s','6s']]
# y_bat = Train_bat["IsInD11"]

# X_bowl = Train_bowl[['Mat','Inns','Overs','Mdns','Runs','Wkts','Ave','Econ','SR','4','5']]
# y_bowl = Train_bowl["IsInD11"]

# X_bat.shape,X_bowl.shape

# data = []
# for i in y_bowl:
#   if i == '2' or i == '`1':
#     data.append('1')
#   else:
#     data.append(str(i))

# y_bowl = pd.Series(data = data)

# """> # Visualisation"""

# y_bat.value_counts()

# import seaborn as sns
# import matplotlib.pyplot as plt

# sns.set()
# y_bat.hist()
# plt.xlabel('IsInD11')
# plt.ylabel('count')

# y_bowl.value_counts()

# import seaborn as sns
# import matplotlib.pyplot as plt

# sns.set()
# y_bowl.hist()
# plt.xlabel('IsInD11')
# plt.ylabel('count')

# """> # Label Encoder"""

# from sklearn.preprocessing import LabelEncoder

# le = LabelEncoder()

# y_bat = le.fit_transform(y_bat)
# y_bowl = le.fit_transform(y_bowl)

# """> # Feature Engineering and Scaling Pipeline"""

# from sklearn.pipeline import Pipeline
# from sklearn.feature_selection import GenericUnivariateSelect, chi2
# from sklearn.preprocessing import StandardScaler

# pipeline_steps = [
#     ("Generic Univariate Select", GenericUnivariateSelect(chi2, mode='k_best', param=10)),
#     ("Standard Scaler", StandardScaler())
# ]

# pipe = Pipeline(steps = pipeline_steps)

# X_transformed_bat = pipe.fit_transform(X_bat, y_bat)

# file3 = "/content/drive/MyDrive/PROJECT/Saved_Models/pipeline_bat.pickle"
# pickle.dump(pipe, open(file3,"wb"))

# X_transformed_bowl = pipe.fit_transform(X_bowl,y_bowl)

# file4 = "/content/drive/MyDrive/PROJECT/Saved_Models/pipeline_bowl.pickle"
# pickle.dump(pipe, open(file4,"wb"))

# X_bat

# """> # Train Test Split"""

# # Batter training set

# from sklearn.model_selection import train_test_split

# X_bat_train, X_bat_test, y_bat_train, y_bat_test = train_test_split(
#     X_transformed_bat, y_bat, test_size=0.05, random_state=42
# )

# # Bowler training set

# from sklearn.model_selection import train_test_split

# X_bowl_train, X_bowl_test, y_bowl_train, y_bowl_test = train_test_split(
#     X_transformed_bowl, y_bowl, test_size=0.05, random_state=42
# )

# """# **MODEL TRAINING AND TESTING**"""

# # Bat model
# from sklearn.ensemble import GradientBoostingClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import confusion_matrix, classification_report
# import seaborn as sns

# # Define the gradienrt boosting model
# gradient_boosting = GradientBoostingClassifier()

# # Define the parameter grid for hyperparameter tuning
# param_grid = {
#     'loss' : ['log_loss', 'exponential'],
#     'learning_rate' : [0.01, 0.05, 0.1],
#     'n_estimators' : [300, 400, 500],
#     'max_depth' : [4, 6, 8],
#     'criterion' : ['friedman_mse', 'squared_error']
# }

# # Perform grid search with cross-validation
# grid_search_bat = GridSearchCV(gradient_boosting, param_grid, cv=5)
# grid_search_bat.fit(X_bat_train, y_bat_train)

# pred_GBGS_bat = grid_search_bat.predict(X_bat_test)

# print(classification_report(y_bat_test , pred_GBGS_bat))
# cf_mat = confusion_matrix(y_bat_test , pred_GBGS_bat)
# sns.heatmap(cf_mat, fmt='', annot=True)

# grid_search_bat.best_params_

# # Bowl model
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import confusion_matrix, classification_report
# import seaborn as sns

# # Define the random forest model
# random_forest = RandomForestClassifier()

# # Define the parameter grid for hyperparameter tuning
# param_grid = {
#     'n_estimators': [300, 400, 500, 600],
#     'max_depth': [4, 6, 8],
#     'criterion': ["log_loss","entropy","gini"]
# }

# # Perform grid search with cross-validation
# grid_search_bowl = GridSearchCV(random_forest, param_grid, cv=5)
# grid_search_bowl.fit(X_bowl_train, y_bowl_train)

# pred_RFGS_bowl = grid_search_bowl.predict(X_bowl_test)

# print(classification_report(y_bowl_test , pred_RFGS_bowl))
# cf_mat = confusion_matrix(y_bowl_test , pred_RFGS_bowl)
# sns.heatmap(cf_mat, fmt='', annot=True)

# grid_search_bowl.best_params_

# """# **SAVE MODELS**"""

# import pickle

# file1 = "/content/drive/MyDrive/PROJECT/bat_model.pickle"
# pickle.dump(grid_search_bat, open(file1,"wb"))

# file2 = "/content/drive/MyDrive/PROJECT/bowl_model.pickle"
# pickle.dump(grid_search_bowl, open(file2,"wb"))